{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "awvBF6ihUrQG",
        "BZ7uK5FoZb6U",
        "muWUKdKVZoSq",
        "0qTp9VSqb1pM",
        "rIbsLT9ii05T"
      ],
      "authorship_tag": "ABX9TyPl8d2WzaB9Qqw3BPvy0c6T"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Processing Text"
      ],
      "metadata": {
        "id": "QAzEYmZWvhlN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "awvBF6ihUrQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "print(f\"Length of text (total number of characters) = {len(raw_text)}\")\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_iut0F4UvY0",
        "outputId": "03d3fb64-fcb3-475d-86f9-5b51835dbc9e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text (total number of characters) = 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the text into constituent words, punctuations and white spaces:"
      ],
      "metadata": {
        "id": "7O3w5OjsWADA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
        "print(preprocessed[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MfX0v6NVG9f",
        "outputId": "82e55278-cecd-4502-e4da-623d6fa887b4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius', '--', 'though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough', '--', 'so', ' ', 'it', ' ', 'was', ' ', 'no', ' ', 'great', ' ', 'surprise', ' ', 'to', ' ', 'me', ' ', 'to', ' ', 'hear', ' ', 'that', ',', '', ' ', 'in', ' ', 'the', ' ', 'height', ' ', 'of', ' ', 'his', ' ', 'glory', ',', '', ' ', 'he', ' ', 'had', ' ', 'dropped', ' ', 'his', ' ', 'painting', ',', '', ' ', 'married', ' ', 'a', ' ', 'rich', ' ', 'widow', ',', '', ' ', 'and', ' ', 'established', ' ', 'himself', ' ', 'in', ' ', 'a']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing whitespace from the list of words and punctuation:"
      ],
      "metadata": {
        "id": "ZlaGxs5WWXJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = [item for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:10])\n",
        "print(len(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tzqe0JNWQwK",
        "outputId": "24103d2f-0ed6-4658-ebf7-432b12d46bf9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius']\n",
            "4649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we create a vocabulary from the preprocessed textual data."
      ],
      "metadata": {
        "id": "QMIuVq77XUZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary  = sorted(list(set(preprocessed)))\n",
        "vocab_size = len(vocabulary)\n",
        "print(vocabulary[:99])\n",
        "print(f\"Vocabulary size = {vocab_size}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guyKB9CkWgjB",
        "outputId": "dbe04d7f-858f-4d96-dead-399905446f98"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['!', '\"', \"'\", '(', ')', ',', '--', '.', ':', ';', '?', 'A', 'Ah', 'Among', 'And', 'Are', 'Arrt', 'As', 'At', 'Be', 'Begin', 'Burlington', 'But', 'By', 'Carlo', 'Carlo;', 'Chicago', 'Claude', 'Come', 'Croft', 'Destroyed', 'Devonshire', 'Don', 'Dubarry', 'Emperors', 'Florence', 'For', 'Gallery', 'Gideon', 'Gisburn', 'Gisburns', 'Grafton', 'Greek', 'Grindle', 'Grindle:', 'Grindles', 'HAD', 'Had', 'Hang', 'Has', 'He', 'Her', 'Hermia', 'His', 'How', 'I', 'If', 'In', 'It', 'Jack', 'Jove', 'Just', 'Lord', 'Made', 'Miss', 'Money', 'Monte', 'Moon-dancers', 'Mr', 'Mrs', 'My', 'Never', 'No', 'Now', 'Nutley', 'Of', 'Oh', 'On', 'Once', 'Only', 'Or', 'Perhaps', 'Poor', 'Professional', 'Renaissance', 'Rickham', 'Rickham;', 'Riviera', 'Rome', 'Russian', 'Sevres', 'She', 'Stroud', 'Strouds', 'Suddenly', 'That', 'The', 'Then', 'There']\n",
            "Vocabulary size = 1159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {text: integer for integer, text in enumerate(vocabulary)}\n",
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i > 50:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trdiY6wWXf6e",
        "outputId": "a795ae3c-60e7-4ac3-c859-dee1273074f7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Carlo;', 25)\n",
            "('Chicago', 26)\n",
            "('Claude', 27)\n",
            "('Come', 28)\n",
            "('Croft', 29)\n",
            "('Destroyed', 30)\n",
            "('Devonshire', 31)\n",
            "('Don', 32)\n",
            "('Dubarry', 33)\n",
            "('Emperors', 34)\n",
            "('Florence', 35)\n",
            "('For', 36)\n",
            "('Gallery', 37)\n",
            "('Gideon', 38)\n",
            "('Gisburn', 39)\n",
            "('Gisburns', 40)\n",
            "('Grafton', 41)\n",
            "('Greek', 42)\n",
            "('Grindle', 43)\n",
            "('Grindle:', 44)\n",
            "('Grindles', 45)\n",
            "('HAD', 46)\n",
            "('Had', 47)\n",
            "('Hang', 48)\n",
            "('Has', 49)\n",
            "('He', 50)\n",
            "('Her', 51)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a tokenizer:"
      ],
      "metadata": {
        "id": "BZ7uK5FoZb6U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizer:\n",
        "  def __init__(self,vocab):\n",
        "    self.text_to_int = vocab\n",
        "    self.int_to_text = {integer:text for  text, integer in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item for item in preprocessed if item.strip()]\n",
        "    ids = [self.text_to_int[item] for item in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_text[i] for i in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "QPz7eVC2YOK6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "testing the tokenizer"
      ],
      "metadata": {
        "id": "VYEtHnTwa2AP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizer(vocab)\n",
        "\n",
        "text = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)\n",
        "text = tokenizer.decode(ids)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoo7Rcdla0t3",
        "outputId": "985f3f57-307f-43f1-dea5-04d3ea755193"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7, 39, 873, 1136, 773, 812, 7]\n",
            "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text = \"Hello, how are you doing ?\"\n",
        "# ids = tokenizer.encode(text)\n",
        "# print(ids)\n",
        "# text = tokenizer.decode(ids)\n",
        "# print(text)"
      ],
      "metadata": {
        "id": "3EzACvrPa7tt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code gives an error when it finds a word that wasn't previously in its vocabulary. To fix this, we add a term in the vocabulary: <|unk|> this handles all the unknown words in the vocabulary."
      ],
      "metadata": {
        "id": "s2bfSRH_nfiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Updating the vocabulary:\n",
        "vocabulary.extend([\"<|unk|>\", \"<|endoftext|>\"])\n",
        "vocab = {text: integer for integer, text in enumerate(vocabulary)}\n",
        "\n",
        "#Creating a new and improved tokenizer:\n",
        "class SimpleTokenizerV2:\n",
        "  def __init__(self, vocab):\n",
        "    self.int_to_text = {i:s for s, i in vocab.items()}\n",
        "    self.text_to_int = vocab\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    preprocessed = [item if item in self.text_to_int else \"<|unk|>\" for item in preprocessed]\n",
        "    ids = [self.text_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_text[s] for s in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "Abe4Z5GHnew1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing this new tokenizer"
      ],
      "metadata": {
        "id": "45qAZIkNqXme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "text1 = \"Hello!, how are you ?\"\n",
        "text2 = \"I am fine, what about you ?\"\n",
        "text = \" <|endoftext|> \".join([text1, text2])\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)\n",
        "decoded = tokenizer.decode(ids)\n",
        "print(decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9BIp-28n1y0",
        "outputId": "d306414e-b99d-4c84-a3f9-ae3898a6de3f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1159, 0, 5, 571, 174, 1155, 10, 1160, 55, 155, 1159, 5, 1116, 122, 1155, 10]\n",
            "<|unk|>!, how are you? <|endoftext|> I am <|unk|>, what about you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Dataset class"
      ],
      "metadata": {
        "id": "muWUKdKVZoSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "  def __init__(self, txt, tokenizer, max_length, stride):\n",
        "    \"\"\"txt: raw text data (string)\n",
        "    tokenizer: object with encode() method to turn text into token IDs\n",
        "    max_length: length of each training sequence\n",
        "    stride: controls overlap between sequences\"\"\"\n",
        "    self.tokenizer = tokenizer\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "    token_ids = tokenizer.encode(txt)\n",
        "    for i in range(0, len(token_ids) - max_length, stride):\n",
        "      '''Slides a window over the long tokenized text.\n",
        "      Starts at i = 0, steps by stride.\n",
        "      Stops before len(token_ids) - max_length to avoid overflow.\n",
        "      This gives overlapping sequences if stride < max_length.'''\n",
        "      input_chunk = token_ids[i:i + max_length]\n",
        "      target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "zriGlCsDzx5d"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Dataset Loader for our LLM"
      ],
      "metadata": {
        "id": "0qTp9VSqb1pM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install tiktoken"
      ],
      "metadata": {
        "id": "P81uh5MFgpLQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True):\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "  dataset = GPTDatasetV1(txt, tokenizer, max_length, stride) #B\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last) #C\n",
        "  return dataloader\n"
      ],
      "metadata": {
        "id": "463JgjiZb1UH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "first_batch = next(data_iter)\n",
        "print(first_batch)\n",
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48Z5amB0g63i",
        "outputId": "ad5db351-fde3-4a90-8f89-20b4091ecec1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Token Embeddings"
      ],
      "metadata": {
        "id": "rIbsLT9ii05T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 6\n",
        "output_dim = 3 # How many vectors we want to use to represent each token\n",
        "\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0mIpm3rhdwj",
        "outputId": "b1f0e2a7-ddb2-47c4-fd63-629ed513cbc7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the weight matrix of the embedding layer contains small, random values. These values are optimized during LLM training as part of the LLM optimization itself, as we will see in upcoming chapters. Moreover, we can see that the weight matrix has six rows and three columns. There is one row for each of the six possible tokens in the vocabulary. And there is one column for each of the three embedding dimensions."
      ],
      "metadata": {
        "id": "H3drVTDy81Un"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(torch.tensor([1,2,3])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPqwJ4I984C7",
        "outputId": "64472d4d-4772-41ac-9c14-6b0137e7700f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vector embeddings for 1 is [ 0.9178,  1.5810,  1.3010] and so on for 2 and 3"
      ],
      "metadata": {
        "id": "NodXgMVd9FxA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This embedding is equal to the row index 1 (second row) in the embedding_layer.weight output"
      ],
      "metadata": {
        "id": "0mpqySsg9SGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding word positions"
      ],
      "metadata": {
        "id": "xQbp0w0N-DnQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the self-attention mechanisms don't have sense of sequence of words, we have to encode that data too"
      ],
      "metadata": {
        "id": "J23kl2kT-R65"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous embedding system, the number 1 will be embedded the same wether it is at the 2nd position or the first."
      ],
      "metadata": {
        "id": "ixPan4FL-d3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"{embedding_layer(torch.tensor([1,2,3]))}\\n--------------\\n{embedding_layer(torch.tensor([2,1,4]))}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xa-6ca3-dhI",
        "outputId": "c0165477-3959-409c-de8c-4951470158e1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n",
            "--------------\n",
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [-1.1589,  0.3255, -0.6315]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So there is no sense of positioning in the embedded data"
      ],
      "metadata": {
        "id": "g5y-MAa-_H_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 50257\n",
        "output_dim = 256\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "print(token_embedding_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bgt2n5yg-HdL",
        "outputId": "531b2a15-8607-495d-fd5f-35694ea2dd40"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[-2.1338,  1.0524, -0.3885,  ...,  0.2461,  1.2119,  0.3171],\n",
            "        [ 1.2277, -0.4297, -2.2121,  ..., -0.1640, -0.3348, -0.0221],\n",
            "        [ 1.3382,  0.2706,  0.5071,  ...,  0.0175, -2.1517,  0.3924],\n",
            "        ...,\n",
            "        [-1.4889, -1.2456,  1.8034,  ..., -0.6392, -1.4939,  0.3614],\n",
            "        [-1.0703,  0.2795, -0.2637,  ..., -0.2810, -1.4755, -0.1183],\n",
            "        [-0.0071,  0.4982, -0.3319,  ...,  0.4970,  0.9365, -0.2091]],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "input, targets = next(data_iter)\n",
        "print(\"Token ID:\\n\", input)\n",
        "print(\"Input shape:\\n\", input.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jw0AFwnRAIIZ",
        "outputId": "29c76797-5aab-4703-a192-d16f1a8ff108"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token ID:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "Input shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_embeddings = token_embedding_layer(input)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWuW6GybBOsp",
        "outputId": "376a6d72-f60a-48c1-957e-8bb4734ad4de"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "#How many tokens the model can see at once\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "# 1 embedding for each position in the sequence, Each embedding is a vector of size output_dim\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
        "# Passes position IDs [0, 1, ..., max_length - 1] through the embedding layer. Gets the corresponding position vectors.\n",
        "print(pos_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LNrn9dSBlkB",
        "outputId": "e832e196-fc16-4e26-f2cc-b2f6220777e4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    }
  ]
}